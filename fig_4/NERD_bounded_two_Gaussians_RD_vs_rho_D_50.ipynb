{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from aux_functions2 import xavier_init\n",
    "from aux_functions2 import plot #MINE\n",
    "\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 20000 # int(sys.argv[1])  # number of training steps\n",
    "iteration = 5 # int(sys.argv[2])  # number of iterations (iid runs)\n",
    "d = 50 # int(sys.argv[3])  # dimension of the distributions\n",
    "mb_size = 40000 # int(sys.argv[4])  # batch size\n",
    "N = 300000\n",
    "rho_range = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "print(steps)\n",
    "print(iteration)\n",
    "print(d)\n",
    "print(mb_size)\n",
    "print(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "if d==4:\n",
    "    layers = [d, 8, 8, 4, 1]\n",
    "elif d==20:\n",
    "    layers = [d, 32, 32, 16, 1]\n",
    "elif d==50:\n",
    "    layers = [d, 32, 32, 16, 1] # [d, 64, 64, 32, 1]\n",
    "else:                      \n",
    "    print('check dimension!')\n",
    "\n",
    "lam = 1.0 # lambda=beta+gamma\n",
    "\n",
    "# initialize\n",
    "X = tf.placeholder(tf.float32, shape=[None, d])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, d])\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    NN_W = []\n",
    "    NN_b = []\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = tf.Variable(xavier_init(size=[layers[l], layers[l+1]]), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        NN_W.append(W)\n",
    "        NN_b.append(b)\n",
    "    return NN_W, NN_b\n",
    "\n",
    "D_W, D_b = initialize_NN(layers)\n",
    "\n",
    "theta_D = [D_W, D_b] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    num_layers = len(D_W) + 1\n",
    "    \n",
    "    h = x  \n",
    "    for l in range(0,num_layers-2):\n",
    "        W = D_W[l]\n",
    "        b = D_b[l]\n",
    "        h = tf.tanh(tf.add(tf.matmul(h, W), b))\n",
    "    \n",
    "    W = D_W[-1]\n",
    "    b = D_b[-1]\n",
    "    out = 50.0 * tf.nn.tanh(tf.add(tf.matmul(h, W), b) / 50.0) # bound M=50\n",
    "    #out = 20.0 * tf.nn.tanh(tf.add(tf.matmul(h, W), b) / 20.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "SF = 1000\n",
    "D_loss_vals =  np.zeros(shape=(len(rho_range), iteration)) \n",
    "RD_exact_rho =  np.zeros(shape= len(rho_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimate Renyi divergence\n",
    "\n",
    "# Loop over #samples, then over iid iterations and then over training steps\n",
    "for j, rho in enumerate(rho_range): \n",
    "    \n",
    "    print('j=', j)\n",
    "    print('rho=', rho)\n",
    "    \n",
    "    # load data\n",
    "    fname = 'data/varying_rho_Sigma1_eye_2/d_' + str(d) + '/input_N' + str(N) + '_dim' + str(d) + '/gaussian_d_' + str(d) + '_'\n",
    "    data = scipy.io.loadmat(fname + 'data_'+str(rho)+'.mat')\n",
    "    x_ = np.array(data['x'])\n",
    "    y_ = np.array(data['y'])\n",
    "\n",
    "    params = scipy.io.loadmat(fname + 'params_'+str(rho)+'.mat')\n",
    "    alpha = np.array(params['alpha'])\n",
    "    No_alpha = alpha.shape[0]\n",
    "\n",
    "    RD_exact = np.array(params['RD_exact']) # contains exact for all alphas. In current datafiles, only one\n",
    "\n",
    "    \n",
    "    beta = lam*(1-alpha[0])  #   <----------- choose the first alpha value\n",
    "    gamma = lam*alpha[0]  #  <---------- choose the first alpha value\n",
    "    \n",
    "    #exact value for Renyi (estimated by integral computation)\n",
    "    RD_exact_rho[j] = RD_exact[0];\n",
    "    \n",
    "    # variational representation:\n",
    "    if beta == 0:\n",
    "        D_loss_real = -tf.reduce_mean(D_real)\n",
    "    else:\n",
    "        max_val = tf.reduce_max((-beta) * D_real)\n",
    "        D_loss_real = (1.0 / beta) * (tf.log(tf.reduce_mean(tf.exp((-beta) * D_real - max_val))) + max_val)\n",
    "\n",
    "    if gamma == 0:\n",
    "        D_loss_fake = tf.reduce_mean(D_fake)\n",
    "\n",
    "    else:\n",
    "        max_val = tf.reduce_max((gamma) * D_fake)\n",
    "        D_loss_fake = (1.0 / gamma) * (tf.log(tf.reduce_mean(tf.exp(gamma * D_fake - max_val))) + max_val)\n",
    "\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    total_loss = D_loss\n",
    "\n",
    "    #D_solver = tf.train.AdamOptimizer().minimize(total_loss, var_list=theta_D)\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(total_loss, var_list=theta_D)\n",
    "\n",
    "    for iter in range(iteration):\n",
    "        print('Iteration: {}'.format(iter))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        x = x_[np.random.randint(x_.shape[0], size=int(0.8*x_.shape[0])), :]\n",
    "        y = y_[np.random.randint(y_.shape[0], size=int(0.8*x_.shape[0])), :]\n",
    "\n",
    "        # initialize for plotting\n",
    "        i = 0\n",
    "        Pl_freq = 10\n",
    "        D_loss_plot = np.zeros(shape=((np.rint(steps / Pl_freq)).astype(int), 1))  # because we writeout every Pl_freq\n",
    "\n",
    "        for it in range(steps):\n",
    "            X_mb = x[np.random.randint(x.shape[0], size=mb_size), :]\n",
    "            Y_mb = y[np.random.randint(y.shape[0], size=mb_size), :]\n",
    "\n",
    "            _, D_loss_curr, D_tot_loss = sess.run([D_solver, D_loss, total_loss], feed_dict={X: X_mb, Y: Y_mb})\n",
    "\n",
    "            if it % Pl_freq == 0:\n",
    "                D_loss_plot[i] = D_loss_curr\n",
    "                i += 1\n",
    "\n",
    "            if it % SF == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('Renyi divergence: {}'.format(-lam*D_loss_curr))\n",
    "                print()\n",
    "\n",
    "\n",
    "        D_loss_curr = sess.run(D_loss, feed_dict={X: x, Y: y})\n",
    "        D_loss_vals[j,iter] = -lam * D_loss_curr\n",
    "\n",
    " \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Plotting\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if not os.path.exists('data/out_gaussian_BS_plots/'):\n",
    "        os.makedirs('data/out_gaussian_BS_plots/')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    #plt.plot(D_loss_plot)\n",
    "    x_idx = np.linspace(0, steps, num=(np.rint(steps / Pl_freq)).astype(int))\n",
    "    plt.plot(x_idx, D_loss_plot)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('D loss')\n",
    "    plt.savefig('data/out_gaussian_BS_plots/cgan_Dloss' + str(j) + 'rho_' + str(rho) +'.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D_loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "with open(fname+'lambda_'+str(lam)+'_bs_'+str(mb_size)+'_nerd_'+str(rho)+'.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in D_loss_vals:\n",
    "        writer.writerow(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RD vs rho\n",
    "#======================\n",
    "fig = plt.figure()\n",
    "plt.plot(rho_range, D_loss_vals[:,0])\n",
    "plt.xlabel('rho')\n",
    "plt.ylabel('divergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('NERD (Bounded)')\n",
    "ax1.boxplot(np.transpose(D_loss_vals[:,:]) , labels=[0.1, 0.3, 0.5, 0.7, 0.9], whis=2)\n",
    "ax1.plot(np.arange(1,len(rho_range)+1), RD_exact_rho[:],'r', 'LineWidth', 2 );\n",
    "plt.xlabel(r'$\\rho$')\n",
    "plt.ylabel(r'$\\mathcal{R}_{\\alpha}(Q||P)$')\n",
    "plt.ylim(0.0, 40.0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('program terminated succesfully')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
