{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from aux_functions2 import xavier_init\n",
    "from aux_functions2 import plot #MINE\n",
    "\n",
    "import csv\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 2000 # int(sys.argv[1])  # number of training steps\n",
    "iteration = 5 # int(sys.argv[2])  # number of iterations (iid runs)\n",
    "d = 1 # int(sys.argv[3])  # dimension of the distributions\n",
    "mb_size = 4000 # int(sys.argv[4])  # batch size\n",
    "N = [5000, 10000, 20000, 50000, 100000, 150000] # t = 0.1 # sys.argv[5]\n",
    "w = 0.2 # subpopulation ratio\n",
    "lam_gp = 0.1  # gradient penalty constant: \\lambda_{gp}\n",
    "\n",
    "print(steps)\n",
    "print(iteration)\n",
    "print(d)\n",
    "print(mb_size)\n",
    "print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-workshop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "if d==2:\n",
    "    layers = [d, 16, 16, 8, 1]\n",
    "elif d==10:\n",
    "    layers = [d, 32, 32, 16, 1]\n",
    "elif d==50:\n",
    "    layers = [d, 64, 64, 32, 1]\n",
    "elif d==1:                      #MINE\n",
    "    layers = [d, 16, 16, 8, 1]# [d, 8, 8, 4, 1] #[d, 4, 4, 2, 1] #\n",
    "\n",
    "lam = 1.0 # lambda=beta+gamma\n",
    "\n",
    "# initialize\n",
    "X = tf.placeholder(tf.float32, shape=[None, d])\n",
    "Y = tf.placeholder(tf.float32, shape=[None, d])\n",
    "\n",
    "def initialize_NN(layers):\n",
    "    NN_W = []\n",
    "    NN_b = []\n",
    "    num_layers = len(layers)\n",
    "    for l in range(0,num_layers-1):\n",
    "        W = tf.Variable(xavier_init(size=[layers[l], layers[l+1]]), name=\"W\")\n",
    "        b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float32), dtype=tf.float32)\n",
    "        NN_W.append(W)\n",
    "        NN_b.append(b)\n",
    "    return NN_W, NN_b\n",
    "\n",
    "D_W, D_b = initialize_NN(layers)\n",
    "\n",
    "theta_D = [D_W, D_b] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    num_layers = len(D_W) + 1\n",
    "    \n",
    "    h = x  #h = [x, x**2] and fix D_w dimension\n",
    "    for l in range(0,num_layers-2):\n",
    "        W = D_W[l]\n",
    "        b = D_b[l]\n",
    "        h = tf.tanh(tf.add(tf.matmul(h, W), b))\n",
    "    \n",
    "    W = D_W[-1]\n",
    "    b = D_b[-1]\n",
    "    #out = 1.0 * tf.nn.tanh(tf.add(tf.matmul(h, W), b) / 1.0)\n",
    "    out =  tf.add(tf.matmul(h, W), b)   # unbounded!\n",
    "\n",
    "    return out\n",
    "\n",
    "D_real = discriminator(X)\n",
    "D_fake = discriminator(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count={'GPU': 0})\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "SF = 1000\n",
    "D_loss_vals =  np.zeros(shape=(len(N), iteration)) # np.zeros(shape=(No_alpha, iteration))  <----------\n",
    "RD_exact_N =  np.zeros(shape= len(N))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimate Renyi divergence\n",
    "\n",
    "# Loop over #samples, then over iid iterations and then over training steps\n",
    "for j, n in enumerate(N): # range(No_alpha):  # <-------------------\n",
    "    \n",
    "    print('j=', j)\n",
    "    print('n=', n)\n",
    "    \n",
    "    # load data\n",
    "    fname = 'data/mu2_1_arch_16/N'+str(n)+'_a_0.5_and_3.7/input_data/GMM_d_1_'\n",
    "    data = scipy.io.loadmat(fname + 'data_'+str(w)+'.mat')\n",
    "    x_ = np.array(data['x'])\n",
    "    y_ = np.array(data['y'])\n",
    "\n",
    "    params = scipy.io.loadmat(fname + 'params_'+str(w)+'.mat')\n",
    "    alpha = np.array(params['alpha'])\n",
    "    No_alpha = alpha.shape[0]\n",
    "\n",
    "    RD_exact = np.array(params['RD_exact']) # contains exact for all alphas\n",
    "\n",
    "    \n",
    "    beta = lam*(1-alpha[0])  #   <----------- choose the first alpha value\n",
    "    gamma = lam*alpha[0]  #  <---------- choose the first alpha value\n",
    "    \n",
    "    #exact value for Renyi (estimated by integral computation)\n",
    "    RD_exact_N[j] = RD_exact[0];\n",
    "    \n",
    "    # variational representation:\n",
    "    if beta == 0:\n",
    "        D_loss_real = -tf.reduce_mean(D_real)\n",
    "    else:\n",
    "        max_val = tf.reduce_max((-beta) * D_real)\n",
    "        D_loss_real = (1.0 / beta) * (tf.log(tf.reduce_mean(tf.exp((-beta) * D_real - max_val))) + max_val)\n",
    "\n",
    "    if gamma == 0:\n",
    "        D_loss_fake = tf.reduce_mean(D_fake)\n",
    "\n",
    "    else:\n",
    "        max_val = tf.reduce_max((gamma) * D_fake)\n",
    "        D_loss_fake = (1.0 / gamma) * (tf.log(tf.reduce_mean(tf.exp(gamma * D_fake - max_val))) + max_val)\n",
    "\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "    \n",
    "    # Gradient Penalty\n",
    "    alpha_gp = tf.random_uniform(shape=[mb_size,1], minval=0., maxval=1.)\n",
    "    interpolates = X + (alpha_gp*(Y - X)) \n",
    "    gradients = tf.gradients(discriminator(interpolates), [interpolates])[0]  \n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients), reduction_indices=[1])) \n",
    "    gradient_penalty = tf.reduce_mean(tf.math.maximum(tf.zeros([slopes.shape[0]], dtype=tf.float32) ,(slopes-1.))**2)  # one-sided penalty  (Lipschitz with k=1)\n",
    "\n",
    "    total_loss = D_loss + lam_gp*gradient_penalty # + because we minimize in optimizer\n",
    "    \n",
    "\n",
    "\n",
    "    D_solver = tf.train.AdamOptimizer(learning_rate=0.0005).minimize(total_loss, var_list=theta_D)\n",
    "\n",
    "    for iter in range(iteration):\n",
    "        print('Iteration: {}'.format(iter))\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        x = x_[np.random.randint(x_.shape[0], size=int(0.8*x_.shape[0])), :]\n",
    "        y = y_[np.random.randint(y_.shape[0], size=int(0.8*x_.shape[0])), :]\n",
    "\n",
    "        # initialize for plotting\n",
    "        i = 0\n",
    "        Pl_freq = 10\n",
    "        D_loss_plot = np.zeros(shape=((np.rint(steps / Pl_freq)).astype(int), 1))  # because we writeout every Pl_freq\n",
    "\n",
    "        for it in range(steps):\n",
    "            X_mb = x[np.random.randint(x.shape[0], size=mb_size), :]\n",
    "            Y_mb = y[np.random.randint(y.shape[0], size=mb_size), :]\n",
    "\n",
    "            _, D_loss_curr, D_tot_loss = sess.run([D_solver, D_loss, total_loss], feed_dict={X: X_mb, Y: Y_mb})\n",
    "\n",
    "            if it % Pl_freq == 0:\n",
    "                D_loss_plot[i] = D_loss_curr\n",
    "                i += 1\n",
    "\n",
    "            if it % SF == 0:\n",
    "                print('Iter: {}'.format(it))\n",
    "                print('Renyi divergence: {}'.format(-lam*D_loss_curr))\n",
    "                print()\n",
    "\n",
    "\n",
    "        D_loss_curr = sess.run(D_loss, feed_dict={X: x, Y: y})\n",
    "        D_loss_vals[j,iter] = -lam * D_loss_curr\n",
    "\n",
    " \n",
    "    # -----------------------------------------------------------------------------\n",
    "    # Plotting\n",
    "    # -----------------------------------------------------------------------------\n",
    "    if not os.path.exists('data/out_GMM_BS_Lip_plots_over_N/'):\n",
    "        os.makedirs('data/out_GMM_BS_Lip_plots_over_N/')\n",
    "\n",
    "    fig = plt.figure()\n",
    "    #plt.plot(D_loss_plot)\n",
    "    x_idx = np.linspace(0, steps, num=(np.rint(steps / Pl_freq)).astype(int))\n",
    "    plt.plot(x_idx, D_loss_plot)\n",
    "    plt.xlabel('steps')\n",
    "    plt.ylabel('D loss')\n",
    "    plt.savefig('data/out_GMM_BS_Lip_plots_over_N/cgan_Dloss' + str(j) + 'w_' + str(w) +'.png', bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "with open(fname+'lambda_'+str(lam)+'_bs_'+str(mb_size)+'_nerd_Lip_'+str(w)+'.csv', \"w\") as output:\n",
    "    writer = csv.writer(output, lineterminator='\\n')\n",
    "    for val in D_loss_vals:\n",
    "        writer.writerow(val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_back = D_loss_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RD vs N\n",
    "#======================\n",
    "fig = plt.figure()\n",
    "#x_idx = np.linspace(0, steps, num=(np.rint(steps / Pl_freq)).astype(int))\n",
    "plt.plot(N, D_loss_vals[:,0])\n",
    "plt.xlabel('N')\n",
    "plt.ylabel('divergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('NERD (Lipschitz)')\n",
    "ax1.plot(np.arange(1,len(N)+1), RD_exact_N[:],'r', 'LineWidth', 2 );\n",
    "ax1.boxplot(np.transpose(D_loss_vals[:,:]) , labels=[5,10,20,50,100,150], whis=2)\n",
    "plt.xlabel(r'$N\\quad [\\times 10^3]$')\n",
    "plt.ylabel(r'$\\mathcal{R}_{\\alpha}(Q||P)$')\n",
    "plt.ylim(0.045, 0.09)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-watson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('program terminated succesfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-bridal",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
